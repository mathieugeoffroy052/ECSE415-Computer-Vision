{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ECSE 415: Final Project\n",
    "Mathieu Geoffroy, 260986559\n",
    "Ryan Reszetnik, 260948454\n",
    "\n",
    "\n",
    "December 5th, 2023"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad2af86b4340174f"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-03T20:44:21.442190Z",
     "start_time": "2023-12-03T20:44:21.440155Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from pathlib import Path\n",
    "\n",
    "working_dir = os.path.curdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "model = YOLO('yolov8n.pt').to(device)\n",
    "\n",
    "# import YOLO labels from the dataset\n",
    "yolo_labels = model.names\n",
    "\n",
    "MODEL_F = 'b0.pth'\n",
    "\n",
    "effnet = EfficientNet.from_pretrained('efficientnet-b0', in_channels=2, num_classes=1).to(device)\n",
    "state = torch.load(MODEL_F, map_location=torch.device(device))\n",
    "effnet.load_state_dict(state)\n",
    "effnet.to(device);"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T20:44:22.696967Z",
     "start_time": "2023-12-03T20:44:21.445707Z"
    }
   },
   "id": "193e7a88fef77a6"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def inference(of_f):\n",
    "    of = np.load(of_f)\n",
    "    # convert to float\n",
    "    of = of.astype(np.float32)\n",
    "    #change shape from (H,W,2) to (2,H,W)\n",
    "    of = np.transpose(of, (2, 0, 1))\n",
    "    #add batch dimension\n",
    "    of = np.expand_dims(of, axis=0)\n",
    "    i = torch.from_numpy(of).to(device)\n",
    "    pred = effnet(i)\n",
    "    del i\n",
    "    torch.mps.empty_cache()\n",
    "    return pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T20:44:22.701112Z",
     "start_time": "2023-12-03T20:44:22.698581Z"
    }
   },
   "id": "fc8b00697a7851b5"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def calc_optical_flow(video_name, frame_index, frame, prev_frame, show_flow=False):\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_frame_gray, frame_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # save the optical flow\n",
    "    np.save(f'{working_dir}/{video_name}/{frame_index}_optical_flow.npy', flow)\n",
    "    \n",
    "    # show the optical flow\n",
    "    if show_flow:\n",
    "        hsv = np.zeros_like(frame)\n",
    "        hsv[...,1] = 255\n",
    "        mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "        hsv[...,0] = ang*180/np.pi/2\n",
    "        hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "        rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n",
    "        cv2.imshow('optical flow',rgb)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T20:44:22.704800Z",
     "start_time": "2023-12-03T20:44:22.701639Z"
    }
   },
   "id": "9c5f7a01c09ea289"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def analyze_video(video_path, display=True, show_flow=False, save=True):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # get video name\n",
    "    video_name = video_path.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    out = cv2.VideoWriter(f'{video_name}_analyzed.mp4',cv2.VideoWriter_fourcc('m','p','4','v'), 30, (int(cap.get(3)),int(cap.get(4))))\n",
    "    \n",
    "    # Store the track history\n",
    "    track_history = defaultdict(lambda: [])\n",
    "    \n",
    "    people_count = 0\n",
    "    car_count = 0\n",
    "    max_speed = 0\n",
    "\n",
    "    frame_index = 0\n",
    "    prev_frame = None\n",
    "    \n",
    "    # Loop through the video frames\n",
    "    while cap.isOpened():\n",
    "        # Read a frame from the video\n",
    "        success, frame = cap.read()\n",
    "    \n",
    "        if success:\n",
    "            start = cv2.getTickCount()  \n",
    "            # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
    "            results = model.track(frame, persist=True, verbose=False)\n",
    "            \n",
    "            # calculate optical flow\n",
    "            if prev_frame is not None:\n",
    "                calc_optical_flow(video_name, frame_index, frame, prev_frame, show_flow)\n",
    "                # created optical flow image, now predict\n",
    "                speed = inference(Path(f'{working_dir}/{video_name}/{frame_index}_optical_flow.npy')).item()\n",
    "                print(f'Frame {frame_index} speed: {round(speed, 2)}')\n",
    "                max_speed = max(max_speed, speed)\n",
    "    \n",
    "            # Get the boxes, track IDs, class, for the frame\n",
    "            boxes = results[0].boxes.xywh.cpu()\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "            classes = results[0].boxes.cls.int().cpu().tolist()\n",
    "    \n",
    "            # Visualize the results on the frame with masks\n",
    "            annotated_frame = results[0].plot()\n",
    "            \n",
    "            # calculate the number of new people and cars\n",
    "            for cls, track_id in zip(classes, track_ids):\n",
    "                if yolo_labels[cls] == 'person' and track_id not in track_history:\n",
    "                    people_count += 1\n",
    "                if yolo_labels[cls] == 'car' and track_id not in track_history:\n",
    "                    car_count += 1\n",
    "                # calculate the speed by tracking stationary objects\n",
    "                # if yolo_labels[cls] == 'traffic light' or yolo_labels[cls] == 'stop sign' or yolo_labels[cls] == 'fire hydrant':\n",
    "                #     if len(track_history[track_id]) > 1:\n",
    "                #         last_point = track_history[track_id][-1]\n",
    "                #         second_last_point = track_history[track_id][-2]\n",
    "                #         distance = np.sqrt((last_point[0] - second_last_point[0])**2 + (last_point[1] - second_last_point[1])**2)\n",
    "                #         # convert pixels to meters\n",
    "                #         distance = distance * 0.0002645833\n",
    "                #         \n",
    "                #         # calculate speed in km/h\n",
    "                #         speed = distance * 3600\n",
    "                #         \n",
    "                #         max_speed = max(max_speed, speed)\n",
    "                    \n",
    "            \n",
    "            # Display the number of people\n",
    "            cv2.putText(annotated_frame, f\"Number of people: {people_count}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "            \n",
    "            # Display the number of cars\n",
    "            cv2.putText(annotated_frame, f\"Number of cars: {car_count}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "            \n",
    "            # Display the speed\n",
    "            cv2.putText(annotated_frame, f\"Max speed: {max_speed} km/h\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)     \n",
    "    \n",
    "            # Plot the tracks\n",
    "            for box, track_id in zip(boxes, track_ids):\n",
    "                x, y, w, h = box\n",
    "                track = track_history[track_id]\n",
    "                track.append((float(x), float(y)))  # x, y center point\n",
    "                if len(track) > 30:  # retain 90 tracks for 90 frames\n",
    "                    track.pop(0)\n",
    "    \n",
    "                # Draw the tracking lines\n",
    "                points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n",
    "                cv2.polylines(annotated_frame, [points], isClosed=False, color=(230, 230, 230), thickness=10)\n",
    "    \n",
    "            # Display the annotated frame\n",
    "            cv2.imshow(f'{video_name} Tracking', annotated_frame) if display else None\n",
    "            \n",
    "            # save the annotated frame to a new video\n",
    "            out.write(annotated_frame) if save else None\n",
    "    \n",
    "            # Break the loop if 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "            # calculate the time it took to process the frame\n",
    "            end = cv2.getTickCount()\n",
    "            time = (end - start)/cv2.getTickFrequency()\n",
    "            \n",
    "            # Print the time it took to process the frame\n",
    "            print(f\"Frame {frame_index} took {time} seconds to process\")\n",
    "            \n",
    "            # increment the frame index\n",
    "            frame_index += 1\n",
    "            prev_frame = frame\n",
    "        else:\n",
    "            # Break the loop if the end of the video is reached\n",
    "            break\n",
    "    \n",
    "    # Release the video capture object and close the display window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return people_count, car_count, max_speed   "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T20:46:13.635017Z",
     "start_time": "2023-12-03T20:46:13.632907Z"
    }
   },
   "id": "27abe2d87ca33460"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 0 took 0.242956417 seconds to process\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[37], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43manalyze_video\u001B[49m\u001B[43m(\u001B[49m\u001B[43mworking_dir\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/mcgill_drive.mp4\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdisplay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshow_flow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m)\n",
      "Cell \u001B[0;32mIn[36], line 31\u001B[0m, in \u001B[0;36manalyze_video\u001B[0;34m(video_path, display, show_flow, save)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# calculate optical flow\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m prev_frame \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 31\u001B[0m     \u001B[43mcalc_optical_flow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvideo_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprev_frame\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshow_flow\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;66;03m# created optical flow image, now predict\u001B[39;00m\n\u001B[1;32m     33\u001B[0m     speed \u001B[38;5;241m=\u001B[39m inference(Path(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mworking_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvideo_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mframe_index\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_optical_flow.npy\u001B[39m\u001B[38;5;124m'\u001B[39m))\u001B[38;5;241m.\u001B[39mitem()\n",
      "Cell \u001B[0;32mIn[29], line 4\u001B[0m, in \u001B[0;36mcalc_optical_flow\u001B[0;34m(video_name, frame_index, frame, prev_frame, show_flow)\u001B[0m\n\u001B[1;32m      2\u001B[0m prev_frame_gray \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(prev_frame, cv2\u001B[38;5;241m.\u001B[39mCOLOR_BGR2GRAY)\n\u001B[1;32m      3\u001B[0m frame_gray \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(frame, cv2\u001B[38;5;241m.\u001B[39mCOLOR_BGR2GRAY)\n\u001B[0;32m----> 4\u001B[0m flow \u001B[38;5;241m=\u001B[39m \u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcalcOpticalFlowFarneback\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprev_frame_gray\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe_gray\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m15\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# save the optical flow\u001B[39;00m\n\u001B[1;32m      6\u001B[0m np\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mworking_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvideo_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mframe_index\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_optical_flow.npy\u001B[39m\u001B[38;5;124m'\u001B[39m, flow)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "print(analyze_video(working_dir + '/mcgill_drive.mp4', display=True, show_flow=False, save=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T20:46:21.107132Z",
     "start_time": "2023-12-03T20:46:20.210353Z"
    }
   },
   "id": "4537606420259054"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(analyze_video(working_dir + '/st-catherines_drive.mp4', display=False, save=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-03T20:44:23.275516Z"
    }
   },
   "id": "1645367a6b6a9a5c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "591785f00609197f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
